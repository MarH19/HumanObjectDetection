{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy\n",
    "#%pip install pandas\n",
    "#%pip install ipykernel\n",
    "#%pip install plotly\n",
    "#%pip install nbformat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#raw data folder name\n",
    "_rawData = '/rawData/'\n",
    "#labeled dataset folder name \n",
    "_labeledData = '/labeledData/'\n",
    "\n",
    "# Path to data\n",
    "dataPath = os.getcwd()+'/dataset/'\n",
    "rawDataPath = dataPath + _rawData\n",
    "labeledDataPath = dataPath + _labeledData\n",
    "saveDatesetPath = dataPath + '/output/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and syncronize data \n",
    "it extact data from each trial and save it to \n",
    "    /dataset/labeledData/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "class MakeFolderDataset:\n",
    "    def __init__(self, folder_path:str) -> None:\n",
    "        self.path = folder_path\n",
    "        self.save_path = folder_path.replace(_rawData, _labeledData )\n",
    "        #os.makedirs(self.save_path)\n",
    "        self.num_lines_per_message = 130\n",
    "        self.df = pd.DataFrame()\n",
    "        self.df_dataset = pd.DataFrame()\n",
    "        self.true_label = pd.DataFrame()\n",
    "        self.tau = ['tau_J0','tau_J1', 'tau_J2', 'tau_J3', 'tau_J4', 'tau_J5', 'tau_J6']\n",
    "        self.tau_d = ['tau_J_d0','tau_J_d1', 'tau_J_d2', 'tau_J_d3', 'tau_J_d4', 'tau_J_d5', 'tau_J_d6']\n",
    "        self.tau_ext =['tau_ext0','tau_ext1','tau_ext2','tau_ext3','tau_ext4','tau_ext5','tau_ext6']\n",
    "\n",
    "        self.q = ['q0','q1','q2','q3','q4','q5','q6']\n",
    "        self.q_d = ['q_d0','q_d1','q_d2','q_d3','q_d4','q_d5','q_d6']\n",
    "\n",
    "        self.dq = ['dq0','dq1','dq2','dq3','dq4','dq5','dq6']\n",
    "        self.dq_d = ['dq_d0','dq_d1','dq_d2','dq_d3','dq_d4','dq_d5','dq_d6']\n",
    "\n",
    "\n",
    "        self.e = ['e0','e1','e2','e3','e4','e5','e6']\n",
    "        self.de = ['de0','de1','de2','de3','de4','de5','de6']\n",
    "        self.etau = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']\n",
    "    \n",
    "    def _extract_array(self, data_dict:dict, data_frame:str, header:list,  n:int):\n",
    "            _, y = data_frame[n].split(':')\n",
    "            y = y.replace('[','')\n",
    "            y = y.replace(']','')\n",
    "            y = y.replace('\\n','')\n",
    "            y = y.split(',')\n",
    "            for i, h in enumerate(header):\n",
    "                data_dict[h].append(float(y[i]))\n",
    "\n",
    "    def extract_robot_data(self):\n",
    "        # it extracts robot data from all_data.txt\n",
    "        f = open(self.path + 'all_data.txt', 'r')\n",
    "        lines = f.readlines()\n",
    "\n",
    "        keywords = ['time'] + self.tau + self.tau_d + self.tau_ext + self.q + self.q_d + self.dq + self.dq_d \n",
    "        data_dict = dict.fromkeys(keywords)\n",
    "        for i in keywords:\n",
    "            data_dict[i]=[0]\n",
    "        \n",
    "        for i in range(int(len(lines)/self.num_lines_per_message)):\n",
    "            data_frame = lines[i*self.num_lines_per_message:(i+1)*self.num_lines_per_message]\n",
    "            \n",
    "            # extract exact time from file sec + nsec\n",
    "            _, y = data_frame[3].split(':')\n",
    "            time_ = int(y)-int(int(y)/1000000)*1000000\n",
    "\n",
    "            _, y = data_frame[4].split(':')\n",
    "            time_ = time_+int(y)/np.power(10,9)\n",
    "\n",
    "            data_dict['time'].append(time_)\n",
    "            \n",
    "            self._extract_array(data_dict,data_frame,self.tau, 25)\n",
    "            self._extract_array(data_dict,data_frame,self.tau_d, 26)\n",
    "            self._extract_array(data_dict,data_frame, self.tau_ext, 37)\n",
    "            \n",
    "            self._extract_array(data_dict,data_frame,self.q, 28)\n",
    "            \n",
    "            self._extract_array(data_dict,data_frame, self.q_d, 29)\n",
    "            self._extract_array(data_dict,data_frame, self.dq, 30)\n",
    "            self._extract_array(data_dict,data_frame, self.dq_d, 31)\n",
    "        \n",
    "       \n",
    "        self.df = pd.DataFrame.from_dict(data_dict)\n",
    "        self.df = self.df.drop(index=0).reset_index()\n",
    "        for i in range(len(self.e)):\n",
    "            self.df[self.e[i]] = self.df[self.q_d[i]]-self.df[self.q[i]]\n",
    "            self.df[self.de[i]] = self.df[self.dq_d[i]]-self.df[self.dq[i]]\n",
    "            self.df[self.etau[i]] = self.df[self.tau_d[i]]-self.df[self.tau[i]]\n",
    "\n",
    "    def get_labels(self):\n",
    "        # it syncronize true labeled (contact- noncontact) data with robot data\n",
    "        true_label = pd.read_csv(self.path+'true_label.csv')\n",
    "        true_label['time'] = true_label['time_sec']+true_label['time_nsec']-self.df['time'][0]\n",
    "        time_dev = true_label['time'].diff()\n",
    "        contact_events_index = np.append([0], true_label['time'][time_dev>0.05].index.values)\n",
    "        contact_events_index = np.append(contact_events_index,  true_label['time'].shape[0]-1)\n",
    "\n",
    "        self.df['time'] = self.df['time'] - self.df['time'][0]\n",
    "        contact_count = 0\n",
    "        self.df['label']=0\n",
    "\n",
    "        for i in range(self.df['time'].shape[0]):\n",
    "            if (self.df['time'][i]-true_label['time'][contact_events_index[contact_count]]) > 0:\n",
    "                #print(i ,',', contact_events_index[contact_count], ',',self.df['time'][i], '   ', true_label['time'][contact_events_index[contact_count]] )\n",
    "                contact_count += 1\n",
    "                if contact_count == len(contact_events_index):\n",
    "                    break\n",
    "                for j in range(i, self.df['time'].shape[0]):\n",
    "                    self.df.loc[j, 'label'] = 1\n",
    "                    #print(j)\n",
    "                    if (self.df['time'][j] - true_label['time'][contact_events_index[contact_count]-1]) > 0:\n",
    "                        #print(j ,',', contact_events_index[contact_count]-1, ',', self.df['time'][j], '   ', true_label['time'][contact_events_index[contact_count]-1] )\n",
    "                        #print('----------------------------------------')\n",
    "                        i = j\n",
    "                        break\n",
    "\n",
    "        self.df.to_csv(self.save_path + 'labeled_data.csv', index=False)\n",
    "\n",
    "    def get_labels_All(self):\n",
    "        self.true_label = pd.read_csv(self.path+'true_label.csv')\n",
    "        self.true_label['time'] = self.true_label['time_sec']+self.true_label['time_nsec']-self.df['time'][0]\n",
    "        self.df['time'] = self.df['time'] - self.df['time'][0]\n",
    "        contact_count = 0\n",
    "        self.df['label']= 0\n",
    "      \n",
    "        \n",
    "        self.true_label['time_dev'] = self.true_label['time'].diff()\n",
    "        \n",
    "        # Find indices where 'time_dev' is greater than 0.05\n",
    "        indices = self.true_label[self.true_label['time_dev'] > 0.05].index\n",
    "\n",
    "        # Iterate through indices in reverse order to avoid index shifting issues\n",
    "        for i in indices[::-1]:\n",
    "     \n",
    "            # Create a new row with the same values except for DATA0 which is set to 0\n",
    "            new_row = self.true_label.iloc[i].to_frame().T\n",
    "            new_row['DATA0'] = 0\n",
    "            \n",
    "            # Concatenate the DataFrame slices properly to avoid duplication\n",
    "            self.true_label = pd.concat([self.true_label.iloc[:i], new_row, self.true_label.iloc[i:]])\n",
    "            \n",
    "\n",
    "        # Reset index after concatenation\n",
    "        self.true_label = self.true_label.reset_index(drop=True)\n",
    "\n",
    "        # Sort the DataFrame by the 'time' column to maintain order\n",
    "        self.true_label = self.true_label.sort_values(by='time')\n",
    "        \n",
    "    def make_sequence(self):\n",
    "        #window_time = 140ms\n",
    "        seq_num = 28\n",
    "        gap = 4\n",
    "        selected_features= self.e + self.tau\n",
    "\n",
    "        dataset = pd.DataFrame(np.ones((int((self.df.shape[0]-seq_num)/gap), seq_num*len(selected_features)+1))*2 )\n",
    "        index = 0\n",
    "        state = False\n",
    "        last_contact_indexes = self.df.loc[self.df['label']==1,'index'].values\n",
    "        last_contact_indexes = last_contact_indexes[last_contact_indexes.shape[0]-1]\n",
    "\n",
    "        for i in range(0, last_contact_indexes, gap):\n",
    "            if state: \n",
    "                window = self.df[selected_features][i:i+seq_num]\n",
    "                dataset.iloc[index,0] = self.df['label'][i+seq_num]\n",
    "                dataset.iloc[index, 1:len(dataset.columns)] = np.hstack(window.to_numpy())\n",
    "                index += 1\n",
    "            else:\n",
    "                if self.df['label'][i+seq_num] == 1:\n",
    "                    state = 1\n",
    "        self.dataset = dataset.drop(index=dataset.loc[dataset[0]==2,0].index)\n",
    "\n",
    "        name = self.path.split('/')[len(self.path.split('/'))-2]+'.csv'\n",
    "        self.dataset.to_csv(self.save_path+name, index=False)\n",
    "        return self.dataset\n",
    "    \n",
    "    def split_data(self, train_split_rate = 0.75):\n",
    "        msk = np.random.rand(len(self.dataset)) < train_split_rate\n",
    "        train = self.dataset.loc[msk, :]\n",
    "        test = self.dataset.loc[~msk, :]\n",
    "        name = self.path.split('/')[len(self.path.split('/'))-2]+'_train.csv'\n",
    "        train.to_csv(self.save_path+name, index=False)\n",
    "\n",
    "        name = self.path.split('/')[len(self.path.split('/'))-2]+'_test.csv'\n",
    "        test.to_csv(self.save_path+name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance = MakeFolderDataset(r\"C:\\Users\\marco\\OneDrive - Universität Zürich UZH\\Master_UZH\\master_project\\dataset\\rawData\\configuration2\\hard\\\")\n",
    "#instance = MakeFolderDataset(r\"C:\\Users\\marco\\OneDrive - Universität Zürich UZH\\Master_UZH\\master_project\\dataset\\rawData\\configuration2\\hard\\\"\")\n",
    "instance1 = MakeFolderDataset(\"C:\\\\Users\\\\juhe9\\\\repos\\\\MasterProject\\\\humanObjectDetection\\\\dataLabeling\\\\dataset\\\\rawData\\\\configuration1\\\\multipleHardwall\\\\\")\n",
    "instance1.extract_robot_data()\n",
    "instance1.get_labels_All()\n",
    "\n",
    "instance2 = MakeFolderDataset(\"C:\\\\Users\\\\juhe9\\\\repos\\\\MasterProject\\\\humanObjectDetection\\\\dataLabeling\\\\dataset\\\\rawData\\\\configuration1\\\\multiplesoftwall\\\\\")\n",
    "instance2.extract_robot_data()\n",
    "instance2.get_labels_All()\n",
    "\n",
    "#instance.true_label.head(100)\n",
    "(instance1.true_label[\"DATA0\"] == 1).sum()\n",
    "(instance2.true_label[\"DATA0\"] == 1).sum()\n",
    "\n",
    "instances =[instance1, instance2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIEHE get_labels_All --> DATA0 bei diff > 0.05 row inserted mit DATA0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "target = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']\n",
    "#target = ['e0','e1','e2','e3','e4','e5','e6']\n",
    "#target = ['de0','de1','de2','de3','de4','de5','de6']\n",
    "for i in target:\n",
    "    for inst in instances:\n",
    "        A = inst.df[i].max()-inst.df[i].min()\n",
    "        # label gets scaled otherwise measure and label are not visible properly on plot\n",
    "        inst.true_label['label_scaled']=inst.true_label['DATA0']*A + inst.df[i].min()     #instance.df[i][0] -A/2\n",
    "        # Assuming df1 and df2 are your two DataFrames with x and y data\n",
    "        trace1 = go.Scatter(x=inst.true_label['time'], y=inst.true_label['label_scaled'], name='true label')\n",
    "        trace2 = go.Scatter(x=inst.df['time'], y=inst.df[i], mode='lines', name='robot data')\n",
    "        data = [trace1, trace2]\n",
    "        layout = go.Layout(title=f'{i} (instance {os.path.basename(os.path.normpath(inst.path))})',\n",
    "                        xaxis=dict(title='time(sec)'),\n",
    "                        yaxis=dict(title='Y-axis'))\n",
    "        fig = go.Figure(data=data, layout=layout)\n",
    "        iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in os.listdir(rawDataPath):\n",
    "    if len(i.split('.'))==1:\n",
    "        submain_path = rawDataPath+i+'/'\n",
    "        print(submain_path)\n",
    "        for j in os.listdir(submain_path):\n",
    "            if len(j.split('.'))==1:\n",
    "                instance = MakeFolderDataset(submain_path+j+'/')\n",
    "                instance.extract_robot_data()\n",
    "                instance.get_labels()\n",
    "                \n",
    "instance.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(instance.df[\"label\"] == 0).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import cufflinks\n",
    "# Using plotly + cufflinks in offline mode\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']\n",
    "\n",
    "for i in target:\n",
    "    A = instance.df[i].max()-instance.df[i].min()\n",
    "    # label gets scaled otherwise measure and label are not visible properly on plot\n",
    "    instance.df['label_scaled']=instance.df['label']*A + instance.df[i][0] -A/2\n",
    "    instance.df.iplot(x='time', y= [i, 'label_scaled'], xTitle='time (sec)', yTitle=i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joint position errors gives good plots --> self.e\n",
    "\n",
    "joint velocity errors bad marker not really visible on plot what (wie eine wolke haha)  --> self.de\n",
    "\n",
    "joint torque errors similar to joint position --> self.etau\n",
    "\n",
    "too many arduino labels, increase threshold of touch \n",
    "how fast is our sampling rate per second? maybe to low "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = ['tau_J0','tau_J1', 'tau_J2', 'tau_J3', 'tau_J4', 'tau_J5', 'tau_J6']\n",
    "tau_d = ['tau_J_d0','tau_J_d1', 'tau_J_d2', 'tau_J_d3', 'tau_J_d4', 'tau_J_d5', 'tau_J_d6']\n",
    "tau_ext =['tau_ext0','tau_ext1','tau_ext2','tau_ext3','tau_ext4','tau_ext5','tau_ext6']\n",
    "\n",
    "q = ['q0','q1','q2','q3','q4','q5','q6']\n",
    "q_d = ['q_d0','q_d1','q_d2','q_d3','q_d4','q_d5','q_d6']\n",
    "\n",
    "dq = ['dq0','dq1','dq2','dq3','dq4','dq5','dq6']\n",
    "dq_d = ['dq_d0','dq_d1','dq_d2','dq_d3','dq_d4','dq_d5','dq_d6']\n",
    "\n",
    "\n",
    "e = ['e0','e1','e2','e3','e4','e5','e6']\n",
    "de = ['de0','de1','de2','de3','de4','de5','de6']\n",
    "etau = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, train_split_rate = 0.75):\n",
    "    msk = np.random.rand(len(dataset)) < train_split_rate\n",
    "    train = dataset.loc[msk, :]\n",
    "    test = dataset.loc[~msk, :]\n",
    "    return train, test\n",
    "\n",
    "def make_sequence(df, selected_features, seq_num = 28, gap = 4):\n",
    "    dataset = pd.DataFrame(np.ones((int((df.shape[0]-seq_num)/gap), seq_num*len(selected_features)+1))*2 )\n",
    "    index = 0\n",
    "    state = False\n",
    "    last_contact_indexes = df.loc[df['label']==1,'index'].values\n",
    "    last_contact_indexes = last_contact_indexes[last_contact_indexes.shape[0]-1]\n",
    "\n",
    "    for i in range(0, last_contact_indexes, gap):\n",
    "        if state: \n",
    "            window =df[selected_features][i:i+seq_num]\n",
    "            dataset.iloc[index,0] = df['label'][i+seq_num]\n",
    "            dataset.iloc[index, 1:len(dataset.columns)] = np.hstack(window.to_numpy())\n",
    "            index += 1\n",
    "        else:\n",
    "            if df['label'][i+seq_num] == 1:\n",
    "                state = 1\n",
    "    dataset = dataset.drop(index=dataset.loc[dataset[0]==2,0].index)\n",
    "    return dataset\n",
    "\n",
    "def make_sequence_full_time(df, selected_features, seq_num = 28, gap = 4):\n",
    "    dataset = pd.DataFrame(np.ones((int((df.shape[0]-seq_num)), seq_num*len(selected_features)+1))*2 )\n",
    "    index = 0\n",
    "    last_contact_indexes = df.loc[df['label']==1,'index'].values\n",
    "    last_contact_indexes = last_contact_indexes[last_contact_indexes.shape[0]-1]\n",
    "\n",
    "    for i in range(0, df.shape[0]-seq_num-1, gap):\n",
    "\n",
    "        window =df[selected_features][i:i+seq_num]\n",
    "        dataset.iloc[index,0] = df['label'][i+seq_num]\n",
    "        dataset.iloc[index, 1:len(dataset.columns)] = np.hstack(window.to_numpy())\n",
    "        index += 1\n",
    "\n",
    "    dataset = dataset.drop(index=dataset.loc[dataset[0]==2,0].index)\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a dataset with a set of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(saveDatesetPath)\n",
    "train_split_rate = 0.7\n",
    "\n",
    "selected_features = tau + tau_ext + e + de\n",
    "seq_num = 28\n",
    "\n",
    "dict_label = {'a': 7, 'b':6, 'c':5, 'd':4, 'e':3, 'f':2, 'g':1}\n",
    "columns = range(seq_num*len(selected_features)+1)\n",
    "\n",
    "df_master_master_train = pd.DataFrame(columns=columns)\n",
    "df_master_master_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "    \n",
    "for i in os.listdir(labeledDataPath):\n",
    "    if len(i.split('.'))==1:\n",
    "\n",
    "        df_master_train = pd.DataFrame(columns=columns)\n",
    "        df_master_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "        submain_path = labeledDataPath+i+'/'\n",
    "        print(submain_path)\n",
    "        for tag_name in os.listdir(submain_path):\n",
    "            if len(tag_name.split('.'))==1:\n",
    "                file_path = submain_path+tag_name+'/labeled_data.csv'\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = make_sequence_full_time(df, selected_features,seq_num)\n",
    "                #labeling data\n",
    "                df[0] = df[0]*dict_label[tag_name[0]]\n",
    "                train, test = split_data(df, train_split_rate)\n",
    "                df_master_train = df_master_train.append(train, ignore_index=True)\n",
    "                df_master_test = df_master_test.append(test, ignore_index=True)\n",
    "                \n",
    "        df_master_train.to_pickle(saveDatesetPath+i+'_train.pkl')\n",
    "        df_master_test.to_pickle(saveDatesetPath+i+'_test.pkl')\n",
    "        \n",
    "    df_master_master_train = df_master_master_train.append(df_master_train, ignore_index=True)\n",
    "    df_master_master_test = df_master_master_test.append(df_master_test, ignore_index=True)\n",
    "\n",
    "df_master_master_train.to_pickle(saveDatesetPath+'dataset_train.pkl')\n",
    "df_master_master_test.to_pickle(saveDatesetPath+'dataset_test.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Print Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of all samples: ', df_master_master_train.shape[0]+df_master_master_test.shape[0])\n",
    "print('number of noncontact samples: ', df_master_master_train[0][df_master_master_train[0]==0].shape[0] +  df_master_master_test[0][df_master_master_test[0]==0].shape[0] )\n",
    "print('number of contact samples: ', df_master_master_train[0][df_master_master_train[0]!=0].shape[0] +  df_master_master_test[0][df_master_master_test[0]!=0].shape[0] )\n",
    "print('Link1 samples: ', df_master_master_train[0][df_master_master_train[0]==1].shape[0] +  df_master_master_test[0][df_master_master_test[0]==1].shape[0] )\n",
    "print('Link2 samples: ', df_master_master_train[0][df_master_master_train[0]==2].shape[0] +  df_master_master_test[0][df_master_master_test[0]==2].shape[0] )\n",
    "print('Link3 samples: ', df_master_master_train[0][df_master_master_train[0]==3].shape[0] +  df_master_master_test[0][df_master_master_test[0]==3].shape[0] )\n",
    "print('Link4 samples: ', df_master_master_train[0][df_master_master_train[0]==4].shape[0] +  df_master_master_test[0][df_master_master_test[0]==4].shape[0] )\n",
    "print('Link5 samples: ', df_master_master_train[0][df_master_master_train[0]==5].shape[0] +  df_master_master_test[0][df_master_master_test[0]==5].shape[0] )\n",
    "print('Link6 samples: ', df_master_master_train[0][df_master_master_train[0]==6].shape[0] +  df_master_master_test[0][df_master_master_test[0]==6].shape[0] )\n",
    "print('Link7 samples: ', df_master_master_train[0][df_master_master_train[0]==7].shape[0] +  df_master_master_test[0][df_master_master_test[0]==7].shape[0] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
