{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# set autoreload to reload all external modules automatically (otherwise changes to those modules won't take effect in the notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from _util.make_folder_dataset import MakeFolderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load environment variables from .env file in repo root\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "#DATASET_REPO_ROOT_PATH=<absolute-path-to-dataset-repo-root-folder>\n",
    "dataset_repo_root_path = Path(os.environ.get(\"DATASET_REPO_ROOT_PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True / False to enable / disable data set persisting\n",
    "save_dataset = True\n",
    "use_test_data = True\n",
    "\n",
    "specific_instances = None\n",
    "specific_plot_target = []#[\"q0\", \"q_d0\", \"e0\", \"dq0\", \"dq_d0\", \"de0\"] # plots everything if None, nothing if [], specific targets if [<list-of-targets>]\n",
    "\n",
    "x_y_data_suffix = \"\"\n",
    "x_y_data_suffix = x_y_data_suffix + \"_TESTDATA\" if use_test_data else x_y_data_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset instances\n",
    "# use current git repo root folder as a reliable \"base\" folder. Dataset folders should be under <git-root>\\dataset\\\n",
    "instances: list[MakeFolderDataset] = []\n",
    "raw_data_path = dataset_repo_root_path / (\"rawData\" if use_test_data == False else \"testData\")\n",
    "for p in raw_data_path.iterdir():\n",
    "    if specific_instances is not None and p.name not in specific_instances:\n",
    "        continue\n",
    "    if p.is_dir() and not p.name == \"_ignore\":\n",
    "        instance = MakeFolderDataset(p.absolute())\n",
    "        instance.extract_robot_data()\n",
    "        instance.get_labels_all()\n",
    "        instances.append(instance)\n",
    "\n",
    "instances = sorted(instances, key=lambda i: i.name)\n",
    "\n",
    "print(f\"found {len(instances)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add contact labels from true_label dataframe to robot-data dataframe for all instances\n",
    "for inst in instances:\n",
    "    inst.df = pd.merge_asof(left=inst.df, right=inst.true_label[[\"time\", \"DATA0\"]], on=\"time\", tolerance=0.02)\n",
    "    inst.df.rename(columns={\"DATA0\": \"has_contact\"}, inplace=True)\n",
    "    inst.df[\"has_contact\"] = inst.df[\"has_contact\"].fillna(0)\n",
    "\n",
    "    if inst.df.loc[1, 'has_contact'] == 1:\n",
    "        inst.df.loc[0, 'has_contact'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up faulty (true_label) sensor data\n",
    "# IMPORTANT: assume that the first measurement (time-window with has_contact = 1) is correct and can be used as a reference point to clean the remaining instance\n",
    "# thus it must manually be verified that the first measurement of each instance is indeed correct\n",
    "\n",
    "def get_contact_duration(df, time):\n",
    "    start_time_index = df[(df['time'] < time) & (\n",
    "        df['has_contact'] == 0)].index[-1] + 1\n",
    "    start_time = df.loc[start_time_index, 'time']\n",
    "    try:\n",
    "        end_time_index = df[(df['time'] > time) & (\n",
    "            df['has_contact'] == 0)].index[0] - 1\n",
    "    except IndexError:\n",
    "        # occurs if filtered df above is empty, which means there is no row with has_contact = 0 after specified time\n",
    "        end_time_index = df.index[-1]\n",
    "    end_time = df.loc[end_time_index, 'time']\n",
    "    return (end_time - start_time), start_time, end_time\n",
    "\n",
    "\n",
    "def get_next_contact_time(df, excl_from_time):\n",
    "    # time of contact time-window must always be greater than time of 1st no-contact\n",
    "    # -> exclude any cut-off contact time-windows at start of measurement\n",
    "    first_no_contact_time = df[df['has_contact'] == 0].iloc[0]['time']\n",
    "    filtered_df = df[(df['time'] > first_no_contact_time) & (\n",
    "        df['time'] > excl_from_time) & (df['has_contact'] == 1)]\n",
    "    return (filtered_df.iloc[0]['time'], filtered_df.index[0]) if len(filtered_df) > 0 else (None, None)\n",
    "\n",
    "\n",
    "for inst in instances:\n",
    "    inst.df['has_contact_original'] = inst.df.loc[:, 'has_contact']\n",
    "\n",
    "    # calculate duration of 1st contact time-window\n",
    "    # 1st contact time-window starts at 1st row with has_contact = 1, where a previous row with has_contact = 0 exists\n",
    "    contact_time, _ = get_next_contact_time(inst.df, inst.start_from_time)\n",
    "    reference_duration, reference_start_time, reference_end_time = get_contact_duration(\n",
    "        inst.df, contact_time)\n",
    "\n",
    "    # set has_contact to 0 for all rows before 1st actuall contact time-window\n",
    "    inst.df.loc[inst.df[\"time\"] < contact_time, \"has_contact\"] = 0\n",
    "\n",
    "    # inst.first_contact_start_time = reference_start_time\n",
    "    # inst.window_size = window_size = len(inst.df[(inst.df['time'] >= reference_start_time) & (\n",
    "    #    inst.df['time'] <= reference_end_time)])\n",
    "\n",
    "    # set has_contact to 0 for all contact time-windows with duration outside of [<lower-bound-multiplier>*reference_duration, <upper-bound-multiplier>*reference_duration]\n",
    "    # -> remove faulty time-windows\n",
    "    # multiplier values can be set manually in meta.json, defaults are 0.85 and 1.2\n",
    "    last_contact_end_time = reference_end_time\n",
    "    reference_duration_multiplier_lower = inst.reference_duration_multiplier_lower if inst.reference_duration_multiplier_lower is not None else 0.85\n",
    "    reference_duration_multiplier_upper = inst.reference_duration_multiplier_upper if inst.reference_duration_multiplier_upper is not None else 1.2\n",
    "    while True:\n",
    "        contact_time, _ = get_next_contact_time(inst.df, last_contact_end_time)\n",
    "        if contact_time is None:\n",
    "            break\n",
    "        contact_duration, contact_start_time, contact_end_time = get_contact_duration(\n",
    "            inst.df, contact_time)\n",
    "        if not (reference_duration_multiplier_lower * reference_duration <= contact_duration <= reference_duration_multiplier_upper * reference_duration):\n",
    "            # print(contact_duration, contact_start_time, contact_end_time)\n",
    "            inst.df.loc[(inst.df['time'] >= contact_start_time) & (\n",
    "                inst.df['time'] <= contact_end_time), 'has_contact'] = 0\n",
    "        last_contact_end_time = contact_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_torque = ['tau_J0', 'tau_J1', 'tau_J2','tau_J3', 'tau_J4', 'tau_J5', 'tau_J6']\n",
    "target_position = ['q0', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6']\n",
    "target_velocity = ['dq0', 'dq1', 'dq2', 'dq3', 'dq4', 'dq5', 'dq6']\n",
    "\n",
    "target_torque_err = ['etau_J0','etau_J1', 'etau_J2', 'etau_J3', 'etau_J4', 'etau_J5', 'etau_J6']\n",
    "target_position_err = ['e0','e1','e2','e3','e4','e5','e6']\n",
    "target_velocity_err = ['de0','de1','de2','de3','de4','de5','de6']\n",
    "\n",
    "targets = target_torque + target_position + target_velocity\n",
    "error_targets = target_torque_err + target_position_err + target_velocity_err\n",
    "\n",
    "all_targets = targets + error_targets\n",
    "\n",
    "plot_target = all_targets if specific_plot_target is None else specific_plot_target\n",
    "for inst in instances:\n",
    "    for i in plot_target:\n",
    "        # label gets scaled otherwise measure and label are not visible properly on plot\n",
    "        A = inst.df[i].max()-inst.df[i].min()\n",
    "        inst.df['has_contact_scaled'] = inst.df['has_contact'] * \\\n",
    "            A + inst.df[i].min()\n",
    "        inst.df['has_contact_original_scaled'] = inst.df['has_contact_original'] * \\\n",
    "            A + inst.df[i].min()\n",
    "        # use plotly to make interactive plots\n",
    "        trace_has_contact = go.Scatter(\n",
    "            x=inst.df['time'], y=inst.df['has_contact_scaled'], name='has contact')\n",
    "        trace_has_contact_original = go.Scatter(\n",
    "            x=inst.df['time'], y=inst.df['has_contact_original_scaled'], name='has contact original')\n",
    "        trace_robotdata = go.Scatter(\n",
    "            x=inst.df['time'], y=inst.df[i], mode='lines', name='robot data')\n",
    "        data = [trace_has_contact_original, trace_robotdata, trace_has_contact]\n",
    "        layout = go.Layout(title=f'{i} (instance {os.path.basename(os.path.normpath(inst.path))})',\n",
    "                           xaxis=dict(title='time(sec)'),\n",
    "                           yaxis=dict(title='Y-axis'))\n",
    "        fig = go.Figure(data=data, layout=layout)\n",
    "        iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(X, y, X_filename, y_filename, subfolder_name=\"\"):\n",
    "    processed_data_path = dataset_repo_root_path / \"processedData\"\n",
    "    processed_data_path = processed_data_path / subfolder_name if use_test_data == False else processed_data_path / \"TESTSET\"\n",
    "    X_path = processed_data_path / X_filename\n",
    "    y_path = processed_data_path / y_filename\n",
    "    print(\n",
    "        f\"saving dataset as: {str(X_path.absolute())} / {str(y_path.absolute())}\")\n",
    "    np.save(str(X_path.absolute()), X)\n",
    "    np.save(str(y_path.absolute()), y)\n",
    "\n",
    "\n",
    "# count instances (per contact class) per motion\n",
    "motion_instance_total_counts = {}\n",
    "motion_instance_current_counts = {}\n",
    "for inst in instances:\n",
    "    motion_instance_current_counts[inst.motion] = {\n",
    "        \"soft\": 0, \"hard\": 0, \"pvc_tube\": 0}\n",
    "    total_counts = motion_instance_total_counts.get(\n",
    "        inst.motion, {\"soft\": 0, \"hard\": 0, \"pvc_tube\": 0})\n",
    "    total_counts[inst.contact_type] = total_counts[inst.contact_type] + 1\n",
    "    motion_instance_total_counts[inst.motion] = total_counts\n",
    "\n",
    "print(\"Number of instances per motion:\")\n",
    "for k, v in motion_instance_total_counts.items():\n",
    "    soft_count, hard_count, pvc_count = v[\"soft\"], v[\"hard\"], v[\"pvc_tube\"]\n",
    "    print(f\"{k}: {soft_count} soft, {hard_count} hard, {pvc_count} pvc\")\n",
    "print()\n",
    "\n",
    "# complete dataset: one numpy dataset with samples of all instances\n",
    "X_single_left_offset, y_single_left_offset = [], []\n",
    "X_sliding_left_offset, y_sliding_left_offset = [], []\n",
    "\n",
    "# split dataset: split one instance per motion off as test set, rest as training set\n",
    "X_split_train_single_left_offset, y_split_train_single_left_offset = [], []\n",
    "X_split_test_single_left_offset, y_split_test_single_left_offset = [], []\n",
    "X_split_train_sliding_left_offset, y_split_train_sliding_left_offset = [], []\n",
    "X_split_test_sliding_left_offset, y_split_test_sliding_left_offset = [], []\n",
    "\n",
    "# 40 data points with robot data publish frequency of 200Hz -> 200ms time-windows\n",
    "window_size = 40\n",
    "# -20 data points offset on the left of a time window with freq. of 200Hz -> include data up to 100ms before contact for sliding windows\n",
    "window_left_offset = -20\n",
    "\n",
    "for inst in instances:\n",
    "    contact_end_time = -1\n",
    "    current_nof_samples, max_nof_samples = 0, 3\n",
    "\n",
    "    # append to train split set unless current inst is the last instance per motion and contact class\n",
    "    current_count = motion_instance_current_counts[inst.motion][inst.contact_type]\n",
    "    append_to_train = current_count < (motion_instance_total_counts[\n",
    "        inst.motion][inst.contact_type] - 1)\n",
    "    motion_instance_current_counts[inst.motion][inst.contact_type] = current_count + 1\n",
    "\n",
    "    # take 3 samples per instance to keep class / motion balance\n",
    "    while current_nof_samples < max_nof_samples:\n",
    "        contact_time, contact_time_index = get_next_contact_time(\n",
    "            inst.df, contact_end_time)\n",
    "        if contact_time is None:\n",
    "            break\n",
    "\n",
    "        _, _, contact_end_time = get_contact_duration(inst.df, contact_time)\n",
    "\n",
    "        # append contact time-windows to feature matrices\n",
    "        if contact_time_index + window_left_offset >= 0:\n",
    "            current_nof_samples += 1\n",
    "\n",
    "            # window thats starts left (by defined offset) of time of contact, with fixed window size\n",
    "            left_offset_window = inst.df.iloc[contact_time_index + window_left_offset:contact_time_index +\n",
    "                                              window_left_offset + window_size][all_targets].to_numpy()\n",
    "            # append to complete dataset\n",
    "            X_single_left_offset.append(left_offset_window)\n",
    "            y_single_left_offset.append(inst.contact_type)\n",
    "            # append to train or test split set\n",
    "            if append_to_train:\n",
    "                X_split_train_single_left_offset.append(left_offset_window)\n",
    "                y_split_train_single_left_offset.append(inst.contact_type)\n",
    "            else:\n",
    "                X_split_test_single_left_offset.append(left_offset_window)\n",
    "                y_split_test_single_left_offset.append(inst.contact_type)\n",
    "\n",
    "            # sliding window: start left (by offset) of time of contact, fixed window size\n",
    "            # move sliding window to the right by 4 rows (= 20ms) each step and append until end of contact time-window is reached by right side of sliding window,\n",
    "            # or until offset equals 20 (-> 100ms after initial contact) to avoid including data too late after initial contact\n",
    "            window_current_offset = window_left_offset\n",
    "            sliding_left_offset_windows_x, sliding_left_offset_windows_y = [], []\n",
    "            while inst.df.iloc[contact_time_index + window_current_offset + window_size]['time'] <= contact_end_time and window_current_offset <= 20:\n",
    "                sliding_left_offset_windows_x.append(\n",
    "                    inst.df.iloc[contact_time_index + window_current_offset:contact_time_index + window_current_offset + window_size][all_targets].to_numpy())\n",
    "                sliding_left_offset_windows_y.append(inst.contact_type)\n",
    "                window_current_offset += 4\n",
    "            # append to complete dataset\n",
    "            X_sliding_left_offset = X_sliding_left_offset + sliding_left_offset_windows_x\n",
    "            y_sliding_left_offset = y_sliding_left_offset + sliding_left_offset_windows_y\n",
    "            # append to train or test split set\n",
    "            if append_to_train:\n",
    "                X_split_train_sliding_left_offset = X_split_train_sliding_left_offset + \\\n",
    "                    sliding_left_offset_windows_x\n",
    "                y_split_train_sliding_left_offset = y_split_train_sliding_left_offset + \\\n",
    "                    sliding_left_offset_windows_y\n",
    "            else:\n",
    "                X_split_test_sliding_left_offset = X_split_test_sliding_left_offset + \\\n",
    "                    sliding_left_offset_windows_x\n",
    "                y_split_test_sliding_left_offset = y_split_test_sliding_left_offset + \\\n",
    "                    sliding_left_offset_windows_y\n",
    "\n",
    "\n",
    "def print_shape(X, y, dataset_description):\n",
    "    print(f\"shape of {dataset_description} feature / label matrices: \")\n",
    "    print(np.shape(X))\n",
    "    print(np.shape(y))\n",
    "\n",
    "\n",
    "X_single_left_offset = np.array(X_single_left_offset)\n",
    "print_shape(X_single_left_offset, y_single_left_offset,\n",
    "            \"complete set single left offset\")\n",
    "X_sliding_left_offset = np.array(X_sliding_left_offset)\n",
    "print_shape(X_sliding_left_offset, y_sliding_left_offset,\n",
    "            \"complete set sliding left offset\")\n",
    "\n",
    "X_split_train_single_left_offset = np.array(X_split_train_single_left_offset)\n",
    "print_shape(X_split_train_single_left_offset,\n",
    "            y_split_train_single_left_offset, \"training set single left offset\")\n",
    "X_split_test_single_left_offset = np.array(X_split_test_single_left_offset)\n",
    "print_shape(X_split_test_single_left_offset,\n",
    "            y_split_test_single_left_offset, \"test set single left offset\")\n",
    "\n",
    "X_split_train_sliding_left_offset = np.array(X_split_train_sliding_left_offset)\n",
    "print_shape(X_split_train_sliding_left_offset,\n",
    "            y_split_train_sliding_left_offset, \"training set sliding left offset\")\n",
    "X_split_test_sliding_left_offset = np.array(X_split_test_sliding_left_offset)\n",
    "print_shape(X_split_test_sliding_left_offset,\n",
    "            y_split_test_sliding_left_offset, \"test set sliding left offset\")\n",
    "\n",
    "if save_dataset:\n",
    "    # save complete dataset\n",
    "    subfolder_name_complete, subfolder_name_split = \"complete\", \"test_train_split\"\n",
    "    save_data(X_single_left_offset, y_single_left_offset,\n",
    "              f\"x_single_left_offset{x_y_data_suffix}.npy\", f\"y_single_left_offset{x_y_data_suffix}.npy\", subfolder_name=subfolder_name_complete)\n",
    "    save_data(X_sliding_left_offset, y_sliding_left_offset,\n",
    "              f\"x_sliding_left_offset{x_y_data_suffix}.npy\", f\"y_sliding_left_offset{x_y_data_suffix}.npy\", subfolder_name=subfolder_name_complete)\n",
    "\n",
    "    if use_test_data == False:\n",
    "        save_data(X_split_train_single_left_offset, y_split_train_single_left_offset,\n",
    "                  f\"x_train_single_left_offset{x_y_data_suffix}.npy\", f\"y_train_single_left_offset{x_y_data_suffix}.npy\", subfolder_name=subfolder_name_split)\n",
    "        save_data(X_split_test_single_left_offset, y_split_test_single_left_offset,\n",
    "                  f\"x_test_single_left_offset{x_y_data_suffix}.npy\", f\"y_test_single_left_offset{x_y_data_suffix}.npy\", subfolder_name=subfolder_name_split)\n",
    "    \n",
    "        save_data(X_split_train_sliding_left_offset, y_split_train_sliding_left_offset,\n",
    "                  f\"x_train_sliding_left_offset{x_y_data_suffix}.npy\", f\"y_train_sliding_left_offset{x_y_data_suffix}.npy\", subfolder_name=subfolder_name_split)\n",
    "        save_data(X_split_test_sliding_left_offset, y_split_test_sliding_left_offset,\n",
    "                  f\"x_test_sliding_left_offset{x_y_data_suffix}.npy\", f\"y_test_sliding_left_offset{x_y_data_suffix}.npy\", subfolder_name=subfolder_name_split)\n",
    "\n",
    "    # save targets as .npy / .txt (can be used to filter features later)\n",
    "    np.save(str(dataset_repo_root_path /\n",
    "            \"processedData\" / \"targets.npy\"), all_targets)\n",
    "    with open(str(dataset_repo_root_path / \"processedData\" / \"targets.txt\"), \"w\") as f:\n",
    "        f.write(str(all_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot histogams of targets used in models (etau, e, de)\n",
    "plot_targets = target_torque + target_position_err + target_velocity_err\n",
    "for i, t in enumerate(all_targets):\n",
    "    if t in plot_targets:\n",
    "        fig, ax = plt.subplots(figsize=(15, 3))\n",
    "        ax.hist(X_single_left_offset[:,:, i].flatten(), bins=100)\n",
    "        ax.set_title(t)\n",
    "        plt.show()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
